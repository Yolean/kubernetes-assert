version: '2.4'
services:

  server:
    image: solsson/ystack-runner:cc6234b863b09ec9272c598d5a2431f9f11b0317-k3s@sha256:2f49c334a5c75093ead3826bc5a72556007b9631d4f1c4e10d2e719ca00d43ed
    command:
    - server
    - --https-listen-port=17243
    - --disable-agent
    - --node-name=server
    - --no-deploy=traefik
    - --no-deploy=servicelb
    - --kube-apiserver-arg
    -   service-node-port-range=31720-31729
    tmpfs:
    - /run
    - /var/run
    privileged: true
    environment:
    - K3S_TOKEN=somethingtotallyrandom
    - K3S_KUBECONFIG_OUTPUT=/admin/.kube/kubeconfig.yaml
    - K3S_KUBECONFIG_MODE=666
    expose:
    - 17243
    - 8472
    - 10250
    volumes:
    - k3s-server:/var/lib/rancher/k3s
    - admin:/admin
    mem_limit:  300000000
    memswap_limit: 0

  agent:
    depends_on:
    - server
    image: solsson/ystack-runner:cc6234b863b09ec9272c598d5a2431f9f11b0317-k3s@sha256:2f49c334a5c75093ead3826bc5a72556007b9631d4f1c4e10d2e719ca00d43ed
    tmpfs:
    - /run
    - /var/run
    privileged: true
    environment:
    - K3S_URL=https://server:17243
    - K3S_TOKEN=somethingtotallyrandom
    expose:
    - 8472
    - 10250
    mem_limit: 1500000000
    memswap_limit: 0

  sut:
    depends_on:
    - server
    - agent
    image: solsson/ystack-runner:cc6234b863b09ec9272c598d5a2431f9f11b0317@sha256:ffc2b8ac771af99c9a9dc4215f72285d33bdb318be58d2bb630e4e33266e69d8
    environment:
    - PROMETHEUS_OPERATOR_BASE=github.com/coreos/prometheus-operator/?ref=ec153e0a2007b1a569876e490cb036c30f5d707b
    - KUBECONFIG_WAIT=30
    - KEEP_RUNNING=false
    volumes:
    - admin:/admin
    entrypoint:
    - /bin/bash
    - -cx
    command:
    - |
      mkdir ~/.kube
      until test -f /admin/.kube/kubeconfig.yaml; do
        [ $$KUBECONFIG_WAIT -gt 0 ] || exit ${BULID_EXIT_CODE_ON_NO_CLUSTER:-0}
        KUBECONFIG_WAIT=$$(( $$KUBECONFIG_WAIT - 1 ))
        echo "Waiting for a kubeconfig ..." && sleep 1
      done
      set -e
      cat /admin/.kube/kubeconfig.yaml | sed 's|127.0.0.1|server|' > ~/.kube/config
      kubectl-waitretry --for=condition=Ready node --all
      kubectl get namespace monitoring 2>/dev/null || kubectl create namespace monitoring
      kubectl apply -k $$PROMETHEUS_OPERATOR_BASE
      if [ "$$KEEP_RUNNING" = "true" ]; then
        echo "Will stay running for manual work"
        sleep infinity
      fi
    mem_limit:   80000000
    memswap_limit: 0

  kubernetes-mixin:
    build: ./kubernetes-mixin
    environment:
    - DEBUG=true
    entrypoint:
    - sh
    - -ce
    command:
    - |
      cat << EOF > /kustomize-base/rules.yaml
      apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      metadata:
        name: kubernetes-mixin-rules
      spec:
      EOF
      cat prometheus_rules.yaml | gojsontoyaml | sed 's|^|  |' >> /kustomize-base/rules.yaml

      cat << EOF > /kustomize-base/alerts.yaml
      apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      metadata:
        name: kubernetes-mixin-alerts
      spec:
      EOF
      cat prometheus_alerts.yaml | gojsontoyaml | sed 's|^|  |' >> /kustomize-base/alerts.yaml

      cp -rv dashboards_out/* /dashboards-kustomize-base
      rm /dashboards-kustomize-base/controller-manager.json
      cat << EOF > /dashboards-kustomize-base/kustomization.yaml
      # It should be possible to extend this set of dashboards using the "replace" behavior
      # Meant to be used with kubectl create -k and kubectl replace -k
      # (unless we learn how to get rid of last-applied-configuration with apply)
      generatorOptions:
        disableNameSuffixHash: true
      configMapGenerator:
      - name: kubernetes-mixin-grafana-dashboards
        files:
      EOF
      find /dashboards-kustomize-base -name \*.json | sed 's|.*/\(.*\)|  - \1=\1|' >> /dashboards-kustomize-base/kustomization.yaml
    volumes:
    - ./kubernetes-mixin:/kustomize-base
    - ./kubernetes-mixin-dashboards:/dashboards-kustomize-base

volumes:
  k3s-server: {}
  admin: {}
